{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kosintsev_ag\\documents\\venvs\\steel_melting\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n",
      "c:\\users\\kosintsev_ag\\documents\\venvs\\steel_melting\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "\n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "\n",
    "    assert x.shape == (2,)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.020281330641362"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kosintsev_ag\\Downloads\\deeplearning_course-Task-1_part-1 (2)\\deeplearning_course-Task-1_part-1\\assignment1\\linear_classifer.py:86: RuntimeWarning: divide by zero encountered in log\n",
      "  loss[idx] = - np.sum(1 * np.log(probs[idx][target_index[idx]]))  # cross-entropy\n",
      "c:\\users\\kosintsev_ag\\documents\\venvs\\steel_melting\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kosintsev_ag\\documents\\venvs\\steel_melting\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n",
      "c:\\users\\kosintsev_ag\\documents\\venvs\\steel_melting\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  import sys\n",
      "c:\\users\\kosintsev_ag\\documents\\venvs\\steel_melting\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  del sys.path[0]\n",
      "c:\\users\\kosintsev_ag\\documents\\venvs\\steel_melting\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kosintsev_ag\\documents\\venvs\\steel_melting\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n",
      "c:\\users\\kosintsev_ag\\documents\\venvs\\steel_melting\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  import sys\n",
      "c:\\users\\kosintsev_ag\\documents\\venvs\\steel_melting\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6096035189662548\n",
      "13.458892931871105\n",
      "33.27373485742093\n",
      "77.29637564878557\n",
      "128.40125977515132\n",
      "179.27356716496658\n",
      "264.4063641092892\n",
      "369.091173902069\n",
      "462.5896668490303\n",
      "589.510578748942\n",
      "706.887689675389\n",
      "798.2055080713727\n",
      "883.4526722051916\n",
      "1021.7230316042688\n",
      "1152.0304822177538\n",
      "1299.8988068420501\n",
      "1428.085905233551\n",
      "1584.425441101786\n",
      "1730.6754900019941\n",
      "1886.5850691106589\n",
      "2035.2353027897166\n",
      "2198.228239691308\n",
      "2363.265357937542\n",
      "2527.4704109069858\n",
      "2696.329733164756\n",
      "2872.630258511024\n",
      "3056.636577166519\n",
      "3229.553281094944\n",
      "3411.2753729492865\n",
      "3587.6279533755705\n",
      "Epoch 0, loss: 42590.133801\n",
      "3769.8924269065633\n",
      "3953.924648004841\n",
      "4139.68617156155\n",
      "4325.203966045461\n",
      "4509.151407480053\n",
      "4696.437002589884\n",
      "4880.774125605431\n",
      "5069.308735426129\n",
      "5260.549692550999\n",
      "5448.365075843563\n",
      "5635.825137487479\n",
      "5817.758717160001\n",
      "5994.015687319501\n",
      "6172.328013826836\n",
      "6339.103248095787\n",
      "6517.306680065872\n",
      "6692.676107579656\n",
      "6870.6424740171515\n",
      "7044.62411729546\n",
      "7216.738502001762\n",
      "7391.568780148799\n",
      "7554.257962375354\n",
      "7725.484080087408\n",
      "7890.377926818229\n",
      "8049.606741766763\n",
      "8218.019910772948\n",
      "8376.614925696465\n",
      "8535.823081641458\n",
      "8699.442788001614\n",
      "8847.687364070294\n",
      "Epoch 1, loss: 191643.195498\n",
      "9002.681485174544\n",
      "9155.228082850874\n",
      "9308.99208231522\n",
      "9462.394512955036\n",
      "9607.49538456098\n",
      "9762.516150221281\n",
      "9901.110369822041\n",
      "10039.215400939886\n",
      "10175.557669139042\n",
      "10325.075270092448\n",
      "10454.546451680733\n",
      "10588.697044926937\n",
      "10725.844574213217\n",
      "10870.668566767517\n",
      "10991.045527091646\n",
      "11111.894780989396\n",
      "11242.805907898855\n",
      "11372.072341022333\n",
      "11498.913105205058\n",
      "11615.247823028722\n",
      "11718.665260464457\n",
      "11839.916301980851\n",
      "11957.459256465976\n",
      "12063.946713871523\n",
      "12170.782898571164\n",
      "12282.278522191114\n",
      "12388.812244980068\n",
      "12495.01355097883\n",
      "12598.15378566608\n",
      "12703.610086957278\n",
      "Epoch 2, loss: 329430.641153\n",
      "12801.072913345895\n",
      "12916.016328958012\n",
      "13021.957722762343\n",
      "13111.695218512106\n",
      "13199.323876720757\n",
      "13288.107983445516\n",
      "13379.01684858466\n",
      "13464.645026986005\n",
      "13553.902358365985\n",
      "13639.86844983375\n",
      "13727.95659418503\n",
      "13814.179078010837\n",
      "13893.729406243\n",
      "13974.433512777812\n",
      "14052.45515032471\n",
      "14129.05210754732\n",
      "14205.386289166416\n",
      "14281.059496087886\n",
      "14354.218968360392\n",
      "14428.239469075002\n",
      "14499.921156054492\n",
      "14569.656290529856\n",
      "14645.95334310745\n",
      "14709.298920894364\n",
      "14778.51924376689\n",
      "14841.899277676283\n",
      "14906.444312196345\n",
      "14969.657576281814\n",
      "15030.707810355283\n",
      "15099.602053378414\n",
      "Epoch 3, loss: 421287.976784\n",
      "15157.666966472918\n",
      "15219.346701797134\n",
      "15284.411425166496\n",
      "15338.091553080923\n",
      "15393.36017237956\n",
      "15454.557556914224\n",
      "15499.771575769752\n",
      "15561.455523135963\n",
      "15641.593240854154\n",
      "15701.992026119315\n",
      "15757.46896133054\n",
      "15784.721141436497\n",
      "15846.783843235884\n",
      "15896.699489458879\n",
      "15937.122313736134\n",
      "15975.226494244987\n",
      "16017.830685203622\n",
      "16051.117101831847\n",
      "16083.732920015125\n",
      "16131.784904766877\n",
      "16180.268618547467\n",
      "16215.515088595625\n",
      "16258.64054559811\n",
      "16284.665500748413\n",
      "16326.528206849298\n",
      "16364.220575488705\n",
      "16400.449008462027\n",
      "16420.52167479029\n",
      "16459.500907051348\n",
      "16487.89374727067\n",
      "Epoch 4, loss: 477132.938470\n",
      "16520.19557292033\n",
      "16556.60633757442\n",
      "16587.076865488245\n",
      "16619.88856881881\n",
      "16650.705505998034\n",
      "16681.531593467866\n",
      "16714.603458020134\n",
      "16744.36080913891\n",
      "16775.039524198517\n",
      "16813.772127210505\n",
      "16846.811878431818\n",
      "16860.447145473066\n",
      "16881.872708636038\n",
      "16908.45672194478\n",
      "16934.86702276231\n",
      "16963.84970967668\n",
      "16987.803724732777\n",
      "17011.757951435222\n",
      "17037.196185949535\n",
      "17063.933275390817\n",
      "17101.986600090764\n",
      "17115.45190365084\n",
      "17132.234233919295\n",
      "17152.249021929347\n",
      "17173.27965579898\n",
      "17195.52181253578\n",
      "17215.52882439087\n",
      "17242.820188251335\n",
      "17259.959181569324\n",
      "17278.701823792257\n",
      "Epoch 5, loss: 508028.509933\n",
      "17300.605297084967\n",
      "17327.10823864642\n",
      "17334.839105201645\n",
      "17353.99743697829\n",
      "17383.514513841023\n",
      "17397.716258719916\n",
      "17437.334878738173\n",
      "17465.73514354849\n",
      "17484.15237708423\n",
      "17480.718718006086\n",
      "17490.71078961095\n",
      "17503.807730974757\n",
      "17514.384784846927\n",
      "17524.96225755094\n",
      "17529.323017708517\n",
      "17546.642263650807\n",
      "17559.621885344543\n",
      "17570.103604983233\n",
      "17580.528064297836\n",
      "17593.058710187142\n",
      "17608.99929793002\n",
      "17619.541457741077\n",
      "17632.86810511593\n",
      "17649.769511823415\n",
      "17665.60016038136\n",
      "17688.71036869141\n",
      "17691.599084847985\n",
      "17691.35566281284\n",
      "17702.970426719006\n",
      "17713.679250997375\n",
      "Epoch 6, loss: 526043.958404\n",
      "17726.58783725116\n",
      "17742.809497559247\n",
      "17759.901986424524\n",
      "17758.214526217016\n",
      "17772.810336383904\n",
      "17795.058971810224\n",
      "17796.793814285957\n",
      "17811.879650136554\n",
      "17823.86561518279\n",
      "17831.773748072777\n",
      "17845.880187780866\n",
      "17846.532996538306\n",
      "17870.208198588043\n",
      "17884.01033124044\n",
      "17873.08534904579\n",
      "17898.010522960918\n",
      "17915.436856783555\n",
      "17893.92037018235\n",
      "17897.922974089804\n",
      "17921.31483301496\n",
      "17934.48348529657\n",
      "17959.547095535512\n",
      "17960.902941557793\n",
      "17954.594818135833\n",
      "17929.41271795565\n",
      "17933.04398100625\n",
      "17962.692702169043\n",
      "17949.376288466254\n",
      "17944.86870543502\n",
      "17952.178691549736\n",
      "Epoch 7, loss: 536147.120031\n",
      "17954.354779387522\n",
      "17958.917221000305\n",
      "17969.129077656085\n",
      "17974.929313737266\n",
      "17989.485524711352\n",
      "18012.676245987474\n",
      "18003.787841896155\n",
      "18004.511889516987\n",
      "18022.592778305814\n",
      "18051.72751296264\n",
      "18038.503808349837\n",
      "18046.97250191648\n",
      "18074.166994716918\n",
      "18068.301054079828\n",
      "18076.9539716806\n",
      "18065.86738720014\n",
      "18057.226488931203\n",
      "18063.065212488997\n",
      "18059.2603256853\n",
      "18059.36831456735\n",
      "18065.161692907932\n",
      "18062.383844381475\n",
      "18066.891296951984\n",
      "18067.308956370434\n",
      "18067.135651544304\n",
      "18072.725453050396\n",
      "18082.35253407999\n",
      "18087.30883101176\n",
      "18087.178978319254\n",
      "18086.091983387396\n",
      "Epoch 8, loss: 541296.337467\n",
      "18096.083996862257\n",
      "18096.070965121533\n",
      "18112.166486898834\n",
      "18112.956557880756\n",
      "18103.402711295017\n",
      "18105.98514957866\n",
      "18130.295741859016\n",
      "18128.324540199123\n",
      "18132.702393262727\n",
      "18145.755272277493\n",
      "18138.981308286297\n",
      "18138.433285729614\n",
      "18179.260068912372\n",
      "18196.64649019123\n",
      "18164.66692177634\n",
      "18183.895819415808\n",
      "18175.892008045543\n",
      "18166.227531313893\n",
      "18186.028914868235\n",
      "18182.054227655954\n",
      "18206.80859371496\n",
      "18191.75286575629\n",
      "18202.189970673156\n",
      "18181.271178029445\n",
      "18172.140601812207\n",
      "18169.804565496375\n",
      "18158.739572331197\n",
      "18156.817733662116\n",
      "18156.590973707345\n",
      "18153.904867277975\n",
      "Epoch 9, loss: 544625.851314\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20392214908>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
