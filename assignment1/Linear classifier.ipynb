{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "\n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "\n",
    "    assert x.shape == (2,)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.020281330641362"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783.9042820227553\n",
      "3188.741489267409\n",
      "10237.693587795347\n",
      "20623.38702574412\n",
      "35644.82087965447\n",
      "54664.87283455777\n",
      "76129.17547321992\n",
      "97071.488664228\n",
      "124232.79588659124\n",
      "154522.95089735172\n",
      "187686.91071913237\n",
      "221786.2366790279\n",
      "259522.0214136596\n",
      "297172.3309378064\n",
      "336982.9335212326\n",
      "378630.5065570615\n",
      "421433.9366759914\n",
      "467522.5963054985\n",
      "514215.8278374344\n",
      "560808.460897184\n",
      "607014.6228863008\n",
      "657052.2588449323\n",
      "705658.6035611096\n",
      "760388.6967744796\n",
      "811022.7378447328\n",
      "862231.0665905377\n",
      "914999.7938635763\n",
      "967958.4025491355\n",
      "1021787.6436872743\n",
      "1076318.7830189713\n",
      "Epoch 0, loss: 12607294.202186\n",
      "1130970.6249390328\n",
      "1186984.7198933703\n",
      "1242469.8567842431\n",
      "1297697.5357587961\n",
      "1354209.8412071948\n",
      "1408353.0263254438\n",
      "1463221.018904178\n",
      "1519632.5698424927\n",
      "1576319.1391008869\n",
      "1634097.0476241745\n",
      "1685895.8674607265\n",
      "1742199.8557800371\n",
      "1802841.096797679\n",
      "1853546.1696317333\n",
      "1911654.911008\n",
      "1962878.9213617027\n",
      "2009216.293532178\n",
      "2059858.8075554334\n",
      "2111780.402815303\n",
      "2163633.5162012083\n",
      "2214794.6180571197\n",
      "2266625.6183175165\n",
      "2317046.471727947\n",
      "2369074.9008535924\n",
      "2422261.902456335\n",
      "2468773.5284197833\n",
      "2516009.3427247903\n",
      "2560557.6788289975\n",
      "2609199.845317705\n",
      "2656452.870835094\n",
      "Epoch 1, loss: 57518258.000063\n",
      "2701630.9506539055\n",
      "2746938.143830499\n",
      "2794328.777043478\n",
      "2839795.4659248115\n",
      "2883390.8699106434\n",
      "2928898.3127153385\n",
      "2971135.15048784\n",
      "3014656.925852179\n",
      "3054510.117193762\n",
      "3095952.2572815856\n",
      "3136799.6381514445\n",
      "3176180.1250520227\n",
      "3215416.854204978\n",
      "3255438.355066514\n",
      "3294691.881968204\n",
      "3333300.437937555\n",
      "3370756.8457527636\n",
      "3407475.5460409224\n",
      "3445719.5379895815\n",
      "3480593.851523919\n",
      "3515006.7919330597\n",
      "3549772.743730492\n",
      "3586357.3568853624\n",
      "3619179.4840370636\n",
      "3652791.237277301\n",
      "3683944.0081515387\n",
      "3715938.638526645\n",
      "3747643.9044168526\n",
      "3779421.7902395143\n",
      "3810250.5156816016\n",
      "Epoch 2, loss: 98807916.515461\n",
      "3842492.7665350633\n",
      "3871639.677202817\n",
      "3900735.9133694787\n",
      "3934214.8322637123\n",
      "3961613.417312201\n",
      "3990881.683038833\n",
      "4022520.6986637246\n",
      "4046436.4014061596\n",
      "4070074.370232712\n",
      "4096524.763742175\n",
      "4124191.2335862764\n",
      "4148046.52323645\n",
      "4173195.359723254\n",
      "4198825.091039412\n",
      "4219974.7830338385\n",
      "4249387.377097312\n",
      "4283404.771865397\n",
      "4308800.679840304\n",
      "4339451.64334944\n",
      "4352126.868087483\n",
      "4388145.170960927\n",
      "4404847.641047543\n",
      "4419478.783039361\n",
      "4429366.11642169\n",
      "4444786.683609435\n",
      "4455654.194481088\n",
      "4475397.18750334\n",
      "4493684.285092144\n",
      "4509386.553608587\n",
      "4527451.622322861\n",
      "Epoch 3, loss: 126682737.092713\n",
      "4545746.819749363\n",
      "4563593.548321389\n",
      "4581502.696469265\n",
      "4599097.045191826\n",
      "4615484.502878923\n",
      "4631998.4121214\n",
      "4649932.912073074\n",
      "4665706.735628875\n",
      "4683389.8891728455\n",
      "4696735.067289124\n",
      "4712527.708808895\n",
      "4725728.038222784\n",
      "4740847.354934429\n",
      "4754639.83553164\n",
      "4768330.393066611\n",
      "4781916.1302946005\n",
      "4795833.525602251\n",
      "4809571.555258855\n",
      "4822453.588013983\n",
      "4835529.047364515\n",
      "4848806.879458148\n",
      "4859288.153020532\n",
      "4871663.292643182\n",
      "4881940.647965406\n",
      "4893445.554527699\n",
      "4905727.37686466\n",
      "4918954.066523494\n",
      "4925779.75347621\n",
      "4942296.800070411\n",
      "4946931.925211297\n",
      "Epoch 4, loss: 142975399.255756\n",
      "4956778.489766883\n",
      "4968871.751998215\n",
      "4980987.439968485\n",
      "4995009.863090464\n",
      "5000066.255777074\n",
      "5008655.086246196\n",
      "5017163.911321677\n",
      "5028270.254730408\n",
      "5037268.882474893\n",
      "5044836.319588823\n",
      "5051839.030686291\n",
      "5061578.639689524\n",
      "5072915.536662329\n",
      "5078711.132959699\n",
      "5087721.680199385\n",
      "5096699.460214829\n",
      "5098856.663041277\n",
      "5105670.164909758\n",
      "5113844.78924856\n",
      "5122521.81456376\n",
      "5132390.856731221\n",
      "5143461.165177298\n",
      "5154771.304105688\n",
      "5153749.359996097\n",
      "5159070.516936187\n",
      "5160374.14387151\n",
      "5165574.59716097\n",
      "5170904.7185509065\n",
      "5176177.419901422\n",
      "5181990.776943442\n",
      "Epoch 5, loss: 152526732.026513\n",
      "5187715.669784673\n",
      "5197204.188018461\n",
      "5201413.737754762\n",
      "5209223.953607602\n",
      "5210219.916063647\n",
      "5215179.863349594\n",
      "5221324.523353804\n",
      "5225655.805295654\n",
      "5233046.232592587\n",
      "5236471.445008757\n",
      "5241591.462182539\n",
      "5246707.494354183\n",
      "5250408.026675872\n",
      "5257038.455122944\n",
      "5260451.27894955\n",
      "5265284.6785821095\n",
      "5268834.585348818\n",
      "5270390.006729011\n",
      "5274444.051235676\n",
      "5279396.083651066\n",
      "5282485.788834375\n",
      "5285738.019455833\n",
      "5289203.091405503\n",
      "5292258.433774881\n",
      "5296064.856120002\n",
      "5300734.645115597\n",
      "5306651.3658382585\n",
      "5308790.090058351\n",
      "5310483.729723029\n",
      "5314174.933401618\n",
      "Epoch 6, loss: 157738586.411389\n",
      "5316234.87152729\n",
      "5319788.772018393\n",
      "5322260.612591022\n",
      "5327618.9611727\n",
      "5327637.331546786\n",
      "5329901.912809137\n",
      "5332381.343540857\n",
      "5335845.459551768\n",
      "5338520.880580075\n",
      "5342410.719989818\n",
      "5348021.29927033\n",
      "5350083.941716061\n",
      "5352865.270690098\n",
      "5360350.726017233\n",
      "5359036.035253761\n",
      "5358561.627439953\n",
      "5361777.142518645\n",
      "5362834.135399874\n",
      "5364938.589486086\n",
      "5368011.597330929\n",
      "5369685.80694312\n",
      "5373245.605786521\n",
      "5372283.36992567\n",
      "5374483.369510476\n",
      "5375943.335761307\n",
      "5377755.098925573\n",
      "5379723.827070117\n",
      "5381482.817363519\n",
      "5383760.5631619515\n",
      "5384292.380408149\n",
      "Epoch 7, loss: 160651737.405307\n",
      "5388299.253153408\n",
      "5389987.858796483\n",
      "5389853.954814885\n",
      "5393887.934233857\n",
      "5393209.835495278\n",
      "5397175.799135983\n",
      "5400999.512227742\n",
      "5400505.594705686\n",
      "5403441.786659712\n",
      "5404810.564443845\n",
      "5405502.00737466\n",
      "5408947.013855115\n",
      "5410277.854278645\n",
      "5411777.561449329\n",
      "5413869.748433057\n",
      "5414057.187514871\n",
      "5413278.804336054\n",
      "5414892.598015256\n",
      "5417652.303014077\n",
      "5419508.630082961\n",
      "5420253.988288068\n",
      "5419295.659791197\n",
      "5421505.9744995525\n",
      "5421212.9681546325\n",
      "5421113.89747008\n",
      "5421874.596306835\n",
      "5428196.45497022\n",
      "5428022.313344134\n",
      "5430760.500341179\n",
      "5426463.728599074\n",
      "Epoch 8, loss: 162330635.883786\n",
      "5427230.929251436\n",
      "5430546.663251864\n",
      "5431809.799205765\n",
      "5431759.7343696095\n",
      "5430303.525171174\n",
      "5431347.328097986\n",
      "5432164.561392178\n",
      "5434452.76812813\n",
      "5434215.593217008\n",
      "5440185.609433502\n",
      "5437692.754163391\n",
      "5436513.096334654\n",
      "5436651.579500236\n",
      "5437155.657526538\n",
      "5437081.335998969\n",
      "5438199.897312561\n",
      "5438101.435145166\n",
      "5439043.605310809\n",
      "5439247.557851754\n",
      "5439464.325004657\n",
      "5440640.923604935\n",
      "5441011.150829624\n",
      "5443019.187300504\n",
      "5446576.427197684\n",
      "5446460.194136341\n",
      "5450507.06021946\n",
      "5448301.159720266\n",
      "5451057.336414012\n",
      "5451980.67493127\n",
      "5448902.163641518\n",
      "Epoch 9, loss: 163171624.033663\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a512126e88>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
